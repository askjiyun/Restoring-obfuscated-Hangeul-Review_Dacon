{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5acb0c5-02b3-48cb-92c6-68b57d1a38fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754416a3-60d5-40d3-84e0-8563287ea33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID                                              input  \\\n",
      "0  TRAIN_00000  별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 섧멍핥쟈...   \n",
      "1  TRAIN_00001                             잚많 쟉꼬 갉 태 좋눼욥. 차못동 줆 ㅋ   \n",
      "2  TRAIN_00002                                    절테 간면 않 된는 굣 멥몫   \n",
      "3  TRAIN_00003  야... 칵컥 좋꾜 부됴 뼝 뚫렷썹 신원햐쥠만 닮패 넴센 밌쪄벅림. 샥퀘 핥류만 묵...   \n",
      "4  TRAIN_00004  집윈 축쳐눌료 딴너왓눈뎁 카셩뷔 좋곱 칼쿰한네올. 쩌럼한뒈 뮬콰 욺료토 잊쿄 빻토 ...   \n",
      "\n",
      "                                              output  \n",
      "0  별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자...  \n",
      "1                             잠만 자고 갈 때 좋네요. 잠옷도 줌 ㅋ  \n",
      "2                                    절대 가면 안 되는 곳 메모  \n",
      "3  아... 가격 좋고 뷰도 뻥 뚫려서 시원하지만 담배 냄새 미쳐버림. 싸게 하루만 묵...  \n",
      "4  지인 추천으로 다녀왔는데 가성비 좋고 깔끔하네요. 저렴한데 물과 음료도 있고 방도 ...  \n"
     ]
    }
   ],
   "source": [
    "train_path=\"../data/train.csv\"\n",
    "train_df = pd.read_csv(train_path)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d38fc3c-bca8-4045-b0d4-88ab81460d8f",
   "metadata": {},
   "source": [
    "- 모델의 System Input으로 \"복원: {난독화된 리뷰}\" 형식으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a471cdf7-b39a-4232-a2a5-1c77126b4d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0  복원: 별 한 게토 았깝땀. 왜 싸람듯릭 펼 1캐를 쥰눈징 컥꺾폰 싸람믐롯섞 맒록 ...   \n",
      "1                         복원: 잚많 쟉꼬 갉 태 좋눼욥. 차못동 줆 ㅋ   \n",
      "2                                복원: 절테 간면 않 된는 굣 멥몫   \n",
      "3  복원: 야... 칵컥 좋꾜 부됴 뼝 뚫렷썹 신원햐쥠만 닮패 넴센 밌쪄벅림. 샥퀘 핥...   \n",
      "4  복원: 집윈 축쳐눌료 딴너왓눈뎁 카셩뷔 좋곱 칼쿰한네올. 쩌럼한뒈 뮬콰 욺료토 잊쿄...   \n",
      "\n",
      "                                         target_text  \n",
      "0  별 한 개도 아깝다. 왜 사람들이 별 1개를 주는지 겪어본 사람으로서 말로 설명하자...  \n",
      "1                             잠만 자고 갈 때 좋네요. 잠옷도 줌 ㅋ  \n",
      "2                                    절대 가면 안 되는 곳 메모  \n",
      "3  아... 가격 좋고 뷰도 뻥 뚫려서 시원하지만 담배 냄새 미쳐버림. 싸게 하루만 묵...  \n",
      "4  지인 추천으로 다녀왔는데 가성비 좋고 깔끔하네요. 저렴한데 물과 음료도 있고 방도 ...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(dataframe, max_input_length=128, max_target_length=128):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe['input_text'] = dataframe['input'].apply(lambda x: f\"복원: {x}\")\n",
    "    dataframe['target_text'] = dataframe['output']\n",
    "    dataframe = dataframe[['input_text', 'target_text']]\n",
    "    return dataframe\n",
    "\n",
    "# Preprocess the training data\n",
    "preprocessed_train_df = preprocess_data(train_df)\n",
    "print(preprocessed_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694f32e-ecde-4a01-abe6-176ffccf4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 데이터 로드\n",
    "train_df = pd.read_csv('../data/train.csv', encoding='utf-8-sig')\n",
    "print(\"Train data shape:\", train_df.shape)\n",
    "print(\"\\nSample data:\")\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9374488-9f7a-46fa-9d22-99ba7915bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 토크나이저 로드 및 분석\n",
    "def analyze_tokenizer(tokenizer, sample_texts):\n",
    "    \"\"\"토크나이저 분석 함수\"\"\"\n",
    "    print(f\"Tokenizer 기본 정보:\")\n",
    "    print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "    print(f\"PAD token: {tokenizer.pad_token}, {tokenizer.pad_token_id}\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token}, {tokenizer.eos_token_id}\")\n",
    "    print(f\"BOS token: {tokenizer.bos_token}, {tokenizer.bos_token_id}\")\n",
    "    \n",
    "    print(\"\\n샘플 텍스트 토큰화 예시:\")\n",
    "    for text in sample_texts:\n",
    "        tokens = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        decoded = tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
    "        print(f\"\\n입력 텍스트: {text}\")\n",
    "        print(f\"토큰화 결과: {decoded}\")\n",
    "        print(f\"토큰 수: {len(decoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a34b27-7b2c-44d9-9f9a-8aa8af8b26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 텍스트로 토크나이저 분석\n",
    "sample_texts = [\n",
    "    train_data['input_text'].iloc[0],\n",
    "    train_data['target_text'].iloc[0]\n",
    "]\n",
    "analyze_tokenizer(tokenizer, sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eecc372e-3081-426d-a6d8-6046cf9bd326",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m token_lengths\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 입력 텍스트와 타겟 텍스트 길이 계산\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m input_lengths \u001b[38;5;241m=\u001b[39m calculate_token_lengths(preprocessed_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mtokenizer\u001b[49m)\n\u001b[1;32m      9\u001b[0m target_lengths \u001b[38;5;241m=\u001b[39m calculate_token_lengths(preprocessed_train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 길이 분포 시각화\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 토큰 길이 계산 함수\n",
    "def calculate_token_lengths(texts, tokenizer):\n",
    "    token_lengths = [len(tokenizer(text, truncation=False)['input_ids']) for text in texts]\n",
    "    return token_lengths\n",
    "\n",
    "# 입력 텍스트와 타겟 텍스트 길이 계산\n",
    "input_lengths = calculate_token_lengths(preprocessed_train_df['input_text'], tokenizer)\n",
    "target_lengths = calculate_token_lengths(preprocessed_train_df['target_text'], tokenizer)\n",
    "\n",
    "# 길이 분포 시각화\n",
    "plt.hist(input_lengths, bins=50, alpha=0.6, label='Input Text Lengths')\n",
    "plt.hist(target_lengths, bins=50, alpha=0.6, label='Target Text Lengths')\n",
    "plt.axvline(x=int(pd.Series(input_lengths).quantile(0.99)), color='blue', linestyle='--', label='95% Input')\n",
    "plt.axvline(x=int(pd.Series(target_lengths).quantile(0.99)), color='orange', linestyle='--', label='95% Target')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9626a-f1a6-433f-bfc1-982f32969614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% 커버 기준 max_length 설정\n",
    "input_max_length = int(pd.Series(input_lengths).quantile(0.99))\n",
    "target_max_length = int(pd.Series(target_lengths).quantile(0.99))\n",
    "print(f\"99% 기준 Input Max Length: {input_max_length}\")\n",
    "print(f\"99% 기준 Target Max Length: {target_max_length}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
